{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc581e4",
   "metadata": {},
   "source": [
    "# 01 - Quick Start: FlowShield-UDRL\n",
    "\n",
    "Ce notebook montre comment utiliser FlowShield-UDRL de manière interactive.\n",
    "\n",
    "## Contenu\n",
    "1. Collecte de données\n",
    "2. Entraînement des modèles\n",
    "3. Évaluation et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6aaf5a",
   "metadata": {},
   "source": [
    "## 1. Collecte de données\n",
    "\n",
    "Collectons des trajectoires avec une politique aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env, n_episodes=100):\n",
    "    \"\"\"Collect trajectories with random policy.\"\"\"\n",
    "    trajectories = []\n",
    "    episode_returns = []\n",
    "    \n",
    "    for _ in tqdm(range(n_episodes), desc='Collecting'):\n",
    "        obs, _ = env.reset()\n",
    "        trajectory = []\n",
    "        episode_return = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            trajectory.append({\n",
    "                'state': obs, 'action': action, 'reward': reward,\n",
    "                'next_state': next_obs, 'done': done\n",
    "            })\n",
    "            episode_return += reward\n",
    "            obs = next_obs\n",
    "        \n",
    "        # Compute hindsight commands\n",
    "        for i, trans in enumerate(trajectory):\n",
    "            horizon = len(trajectory) - i\n",
    "            return_to_go = sum(t['reward'] for t in trajectory[i:])\n",
    "            trans['command'] = np.array([horizon, return_to_go], dtype=np.float32)\n",
    "        \n",
    "        trajectories.extend(trajectory)\n",
    "        episode_returns.append(episode_return)\n",
    "    \n",
    "    return trajectories, episode_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('LunarLander-v3', continuous=True)\n",
    "print(f'State dim: {env.observation_space.shape[0]}')\n",
    "print(f'Action dim: {env.action_space.shape[0]}')\n",
    "\n",
    "# Collect data\n",
    "trajectories, returns = collect_trajectories(env, n_episodes=200)\n",
    "print(f'\\nCollected {len(trajectories)} transitions')\n",
    "print(f'Mean return: {np.mean(returns):.2f} ± {np.std(returns):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44871e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "states = np.array([t['state'] for t in trajectories], dtype=np.float32)\n",
    "actions = np.array([t['action'] for t in trajectories], dtype=np.float32)\n",
    "commands = np.array([t['command'] for t in trajectories], dtype=np.float32)\n",
    "\n",
    "# Visualize command distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(commands[:, 0], bins=50, color='blue', alpha=0.7)\n",
    "axes[0].set_xlabel('Horizon')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Horizon Distribution')\n",
    "\n",
    "axes[1].hist(commands[:, 1], bins=50, color='green', alpha=0.7)\n",
    "axes[1].set_xlabel('Return-to-go')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Return-to-go Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6eca41",
   "metadata": {},
   "source": [
    "## 2. Entraînement des modèles\n",
    "\n",
    "### 2.1 UDRL Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.models import UDRLPolicy, FlowMatchingShield, QuantileShield\n",
    "\n",
    "# Create policy\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "policy = UDRLPolicy(state_dim, action_dim, hidden_dim=128).to(device)\n",
    "print(f'Policy parameters: {sum(p.numel() for p in policy.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Prepare tensors\n",
    "states_t = torch.tensor(states, device=device)\n",
    "actions_t = torch.tensor(actions, device=device)\n",
    "commands_t = torch.tensor(commands, device=device)\n",
    "\n",
    "# Training\n",
    "optimizer = Adam(policy.parameters(), lr=1e-3)\n",
    "batch_size = 256\n",
    "n_epochs = 50\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc='Training Policy'):\n",
    "    indices = np.random.permutation(len(states))\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(states), batch_size):\n",
    "        idx = indices[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        log_prob = policy.log_prob(states_t[idx], commands_t[idx], actions_t[idx])\n",
    "        loss = -log_prob.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    losses.append(epoch_loss / n_batches)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('UDRL Policy Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c9dfe",
   "metadata": {},
   "source": [
    "### 2.2 Flow Matching Shield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b061ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Flow Shield\n",
    "flow_shield = FlowMatchingShield(state_dim, hidden_dim=128).to(device)\n",
    "\n",
    "# Training\n",
    "optimizer = Adam(flow_shield.parameters(), lr=1e-3)\n",
    "flow_losses = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc='Training Flow Shield'):\n",
    "    indices = np.random.permutation(len(states))\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(states), batch_size):\n",
    "        idx = indices[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = flow_shield.loss(states_t[idx], commands_t[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    flow_losses.append(epoch_loss / n_batches)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(flow_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CFM Loss')\n",
    "plt.title('Flow Matching Shield Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9a18e",
   "metadata": {},
   "source": [
    "## 3. Visualisation OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ebb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OOD detection\n",
    "h_range = np.linspace(commands[:, 0].min() - 50, commands[:, 0].max() + 50, 40)\n",
    "r_range = np.linspace(commands[:, 1].min() - 100, commands[:, 1].max() + 100, 40)\n",
    "H, R = np.meshgrid(h_range, r_range)\n",
    "\n",
    "# Use mean state\n",
    "mean_state = states_t.mean(dim=0, keepdim=True).repeat(len(h_range) * len(r_range), 1)\n",
    "grid_commands = torch.tensor(np.stack([H.flatten(), R.flatten()], axis=1), \n",
    "                             dtype=torch.float32, device=device)\n",
    "\n",
    "# Compute OOD\n",
    "with torch.no_grad():\n",
    "    ood = flow_shield.is_ood(mean_state, grid_commands)\n",
    "\n",
    "ood_grid = ood.cpu().numpy().reshape(H.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(H, R, ood_grid, levels=1, colors=['lightgreen', 'lightcoral'], alpha=0.5)\n",
    "plt.contour(H, R, ood_grid, levels=[0.5], colors='red', linewidths=2)\n",
    "plt.scatter(commands[:500, 0], commands[:500, 1], c='blue', s=5, alpha=0.3, label='Training data')\n",
    "plt.xlabel('Horizon (H)')\n",
    "plt.ylabel('Return-to-go (R)')\n",
    "plt.title('Flow Shield OOD Detection')\n",
    "plt.colorbar(label='OOD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ae13d",
   "metadata": {},
   "source": [
    "## 4. Test du Shield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with and without shield\n",
    "def run_episode(policy, env, command, shield=None, device='cpu'):\n",
    "    \"\"\"Run a single episode.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    episode_return = 0\n",
    "    target_h, target_r = command\n",
    "    steps = 0\n",
    "    projections = 0\n",
    "    \n",
    "    while steps < 500:\n",
    "        state_t = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "        cmd_t = torch.tensor([[target_h - steps, target_r - episode_return]], \n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "        if shield is not None:\n",
    "            if shield.is_ood(state_t, cmd_t).any():\n",
    "                cmd_t = shield.project(state_t, cmd_t)\n",
    "                projections += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = policy.sample(state_t, cmd_t, deterministic=True)\n",
    "        action = np.clip(action.cpu().numpy()[0], -1, 1)\n",
    "        \n",
    "        state, reward, term, trunc, _ = env.step(action)\n",
    "        episode_return += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if term or trunc:\n",
    "            break\n",
    "    \n",
    "    return episode_return, steps, projections\n",
    "\n",
    "# Test commands (some OOD)\n",
    "test_commands = [\n",
    "    (100, -200),  # Realistic\n",
    "    (50, 100),    # Ambitious\n",
    "    (30, 250),    # Very OOD\n",
    "]\n",
    "\n",
    "print('Testing without shield:')\n",
    "for cmd in test_commands:\n",
    "    ret, steps, _ = run_episode(policy, env, cmd, shield=None, device=device)\n",
    "    print(f'  Command H={cmd[0]}, R={cmd[1]}: Return={ret:.1f}, Steps={steps}')\n",
    "\n",
    "print('\\nTesting with Flow Shield:')\n",
    "for cmd in test_commands:\n",
    "    ret, steps, proj = run_episode(policy, env, cmd, shield=flow_shield, device=device)\n",
    "    print(f'  Command H={cmd[0]}, R={cmd[1]}: Return={ret:.1f}, Steps={steps}, Projections={proj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
