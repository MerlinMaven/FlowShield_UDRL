{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9741ee9",
   "metadata": {},
   "source": [
    "# Flow Matching Deep Dive\n",
    "\n",
    "This notebook explores the Flow Matching component of FlowShield-UDRL in detail:\n",
    "\n",
    "1. Theory: Continuous Normalizing Flows and Flow Matching\n",
    "2. Vector field learning\n",
    "3. ODE integration for sampling and density estimation\n",
    "4. Visualization of learned flows\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b107de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from src.models.safety.flow_matching import VectorFieldNetwork, FlowMatchingModel\n",
    "from src.utils.seed import set_global_seed\n",
    "\n",
    "set_global_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98a92c",
   "metadata": {},
   "source": [
    "## 1. Theory: Flow Matching\n",
    "\n",
    "Flow Matching learns a **time-dependent vector field** $v_\\theta(x, t)$ that transforms\n",
    "a simple base distribution (e.g., Gaussian) into a complex target distribution.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Probability path**: $p_t(x)$ interpolates between base $p_0(x)$ and target $p_1(x)$\n",
    "\n",
    "2. **Vector field**: $v_t(x)$ describes how samples flow from $t=0$ to $t=1$\n",
    "\n",
    "3. **ODE**: Samples evolve according to $\\frac{dx}{dt} = v_t(x)$\n",
    "\n",
    "4. **Optimal Transport path** (Lipman et al., 2022):\n",
    "   $$x_t = (1-t) x_0 + t x_1$$\n",
    "   $$v_t(x_t | x_1) = x_1 - x_0$$\n",
    "\n",
    "5. **Flow Matching loss**:\n",
    "   $$\\mathcal{L} = \\mathbb{E}_{t, x_0, x_1} \\|v_\\theta(x_t, t) - (x_1 - x_0)\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe5700",
   "metadata": {},
   "source": [
    "## 2. Create a Simple 2D Example\n",
    "\n",
    "Let's start with a simple 2D example to understand the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D target distribution (mixture of Gaussians)\n",
    "def sample_target(n_samples):\n",
    "    \"\"\"Sample from a mixture of Gaussians.\"\"\"\n",
    "    centers = torch.tensor([\n",
    "        [-2.0, 2.0],\n",
    "        [2.0, 2.0],\n",
    "        [0.0, -2.0],\n",
    "    ])\n",
    "    \n",
    "    # Random center selection\n",
    "    idx = torch.randint(0, len(centers), (n_samples,))\n",
    "    samples = centers[idx] + 0.5 * torch.randn(n_samples, 2)\n",
    "    return samples\n",
    "\n",
    "# Visualize target distribution\n",
    "target_samples = sample_target(1000)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], alpha=0.5, s=10)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title('Target Distribution (Mixture of Gaussians)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple vector field network (unconditional)\n",
    "class SimpleVectorField(nn.Module):\n",
    "    def __init__(self, data_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(data_dim + 1, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, data_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        if t.dim() == 0:\n",
    "            t = t.expand(x.shape[0], 1)\n",
    "        elif t.dim() == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        return self.net(torch.cat([x, t], dim=-1))\n",
    "\n",
    "# Create and train\n",
    "model = SimpleVectorField(data_dim=2, hidden_dim=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd460eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with Flow Matching\n",
    "n_epochs = 500\n",
    "batch_size = 256\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sample target (x_1)\n",
    "    x1 = sample_target(batch_size).to(device)\n",
    "    \n",
    "    # Sample base (x_0 ~ N(0, I))\n",
    "    x0 = torch.randn_like(x1)\n",
    "    \n",
    "    # Sample time\n",
    "    t = torch.rand(batch_size, device=device)\n",
    "    \n",
    "    # Interpolate (optimal transport path)\n",
    "    xt = (1 - t.unsqueeze(-1)) * x0 + t.unsqueeze(-1) * x1\n",
    "    \n",
    "    # Target vector field\n",
    "    target_v = x1 - x0\n",
    "    \n",
    "    # Predict vector field\n",
    "    pred_v = model(xt, t)\n",
    "    \n",
    "    # Flow Matching loss\n",
    "    loss = ((pred_v - target_v) ** 2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}: Loss = {loss.item():.6f}')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Flow Matching Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c08795",
   "metadata": {},
   "source": [
    "## 3. Visualize the Learned Vector Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vector_field(model, t, ax, n_grid=20, scale=0.3):\n",
    "    \"\"\"Plot vector field at time t.\"\"\"\n",
    "    x = torch.linspace(-5, 5, n_grid)\n",
    "    y = torch.linspace(-5, 5, n_grid)\n",
    "    X, Y = torch.meshgrid(x, y, indexing='xy')\n",
    "    \n",
    "    grid = torch.stack([X.flatten(), Y.flatten()], dim=1).to(device)\n",
    "    t_tensor = torch.full((grid.shape[0],), t, device=device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v = model(grid, t_tensor)\n",
    "    \n",
    "    # Reshape and plot\n",
    "    vx = v[:, 0].reshape(n_grid, n_grid).cpu().numpy()\n",
    "    vy = v[:, 1].reshape(n_grid, n_grid).cpu().numpy()\n",
    "    \n",
    "    ax.quiver(X.numpy(), Y.numpy(), vx, vy, alpha=0.7, scale=1/scale)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_title(f't = {t:.2f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot vector field at different times\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, t in zip(axes, [0.0, 0.25, 0.5, 0.75]):\n",
    "    plot_vector_field(model, t, ax)\n",
    "\n",
    "plt.suptitle('Learned Vector Field at Different Times', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51336f",
   "metadata": {},
   "source": [
    "## 4. Generate Samples via ODE Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_ode(model, x0, n_steps=100):\n",
    "    \"\"\"Integrate ODE from t=0 to t=1 using Euler method.\"\"\"\n",
    "    dt = 1.0 / n_steps\n",
    "    x = x0.clone()\n",
    "    trajectory = [x.clone()]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_steps):\n",
    "            t = i / n_steps\n",
    "            t_tensor = torch.full((x.shape[0],), t, device=device)\n",
    "            v = model(x, t_tensor)\n",
    "            x = x + dt * v\n",
    "            trajectory.append(x.clone())\n",
    "    \n",
    "    return torch.stack(trajectory, dim=0)\n",
    "\n",
    "# Sample from base distribution\n",
    "n_samples = 500\n",
    "x0 = torch.randn(n_samples, 2).to(device)\n",
    "\n",
    "# Integrate\n",
    "trajectory = integrate_ode(model, x0)\n",
    "\n",
    "print(f'Trajectory shape: {trajectory.shape}')  # [n_steps+1, n_samples, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize flow\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "time_indices = [0, 25, 50, 75, 100]  # t = 0, 0.25, 0.5, 0.75, 1.0\n",
    "\n",
    "for ax, t_idx in zip(axes, time_indices):\n",
    "    samples = trajectory[t_idx].cpu().numpy()\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=10, c='blue')\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_title(f't = {t_idx/100:.2f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "# Add target distribution for comparison\n",
    "target_samples = sample_target(500)\n",
    "axes[-1].scatter(target_samples[:, 0], target_samples[:, 1], alpha=0.3, s=10, c='red', label='Target')\n",
    "axes[-1].legend()\n",
    "\n",
    "plt.suptitle('Flow Evolution: Base → Target', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e90bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample trajectories\n",
    "n_traj = 50\n",
    "x0 = torch.randn(n_traj, 2).to(device)\n",
    "trajectory = integrate_ode(model, x0, n_steps=100)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot trajectories\n",
    "traj_np = trajectory.cpu().numpy()\n",
    "for i in range(n_traj):\n",
    "    plt.plot(traj_np[:, i, 0], traj_np[:, i, 1], alpha=0.3, linewidth=0.5)\n",
    "\n",
    "# Mark start and end points\n",
    "plt.scatter(traj_np[0, :, 0], traj_np[0, :, 1], c='blue', s=30, label='Start (t=0)', zorder=5)\n",
    "plt.scatter(traj_np[-1, :, 0], traj_np[-1, :, 1], c='red', s=30, label='End (t=1)', zorder=5)\n",
    "\n",
    "# Add target centers\n",
    "centers = [[-2, 2], [2, 2], [0, -2]]\n",
    "for c in centers:\n",
    "    plt.scatter(*c, c='green', s=100, marker='x', linewidths=3, label='Target center' if c == centers[0] else '')\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Sample Trajectories from Base to Target')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab62ed1",
   "metadata": {},
   "source": [
    "## 5. Density Estimation with Change of Variables\n",
    "\n",
    "We can compute $\\log p(x_1)$ using the change of variables formula:\n",
    "\n",
    "$$\\log p(x_1) = \\log p(x_0) - \\int_0^1 \\text{tr}\\left(\\frac{\\partial v_t}{\\partial x}\\right) dt$$\n",
    "\n",
    "The Hutchinson trace estimator makes this tractable:\n",
    "\n",
    "$$\\text{tr}(J) = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} [\\epsilon^\\top J \\epsilon]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e268002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(model, x1, n_steps=100, n_trace_samples=10):\n",
    "    \"\"\"Compute log probability of samples using reverse integration.\"\"\"\n",
    "    dt = 1.0 / n_steps\n",
    "    x = x1.clone().requires_grad_(True)\n",
    "    log_det = torch.zeros(x.shape[0], device=device)\n",
    "    \n",
    "    for i in range(n_steps, 0, -1):\n",
    "        t = i / n_steps\n",
    "        t_tensor = torch.full((x.shape[0],), t, device=device)\n",
    "        \n",
    "        # Compute vector field\n",
    "        v = model(x, t_tensor)\n",
    "        \n",
    "        # Hutchinson trace estimator\n",
    "        trace = 0.0\n",
    "        for _ in range(n_trace_samples):\n",
    "            eps = torch.randn_like(x)\n",
    "            eps_grad = torch.autograd.grad(\n",
    "                v, x, eps, create_graph=False, retain_graph=True\n",
    "            )[0]\n",
    "            trace += (eps * eps_grad).sum(dim=-1)\n",
    "        trace = trace / n_trace_samples\n",
    "        \n",
    "        # Update\n",
    "        log_det = log_det + dt * trace\n",
    "        \n",
    "        x = x.detach() - dt * v.detach()\n",
    "        x.requires_grad_(True)\n",
    "    \n",
    "    # Log prob under base distribution (standard Gaussian)\n",
    "    log_p0 = -0.5 * (x.detach() ** 2).sum(dim=-1) - x.shape[-1] * 0.5 * np.log(2 * np.pi)\n",
    "    \n",
    "    return log_p0 + log_det.detach()\n",
    "\n",
    "# Compute log probs for samples\n",
    "test_samples = sample_target(200).to(device)\n",
    "log_probs = compute_log_prob(model, test_samples)\n",
    "\n",
    "print(f'Mean log prob: {log_probs.mean().item():.4f}')\n",
    "print(f'Std log prob: {log_probs.std().item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb017b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize log probability landscape\n",
    "n_grid = 50\n",
    "x = torch.linspace(-5, 5, n_grid)\n",
    "y = torch.linspace(-5, 5, n_grid)\n",
    "X, Y = torch.meshgrid(x, y, indexing='xy')\n",
    "grid = torch.stack([X.flatten(), Y.flatten()], dim=1).to(device)\n",
    "\n",
    "# Compute log probs (this may take a moment)\n",
    "print('Computing log probabilities over grid...')\n",
    "log_probs_grid = compute_log_prob(model, grid, n_steps=50, n_trace_samples=5)\n",
    "log_probs_grid = log_probs_grid.reshape(n_grid, n_grid).cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Log probability\n",
    "im1 = axes[0].imshow(\n",
    "    log_probs_grid.T, origin='lower', aspect='equal',\n",
    "    extent=[-5, 5, -5, 5],\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.colorbar(im1, ax=axes[0], label='log p(x)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Log Probability Density')\n",
    "\n",
    "# Probability\n",
    "prob_grid = np.exp(np.clip(log_probs_grid, -20, 5))\n",
    "im2 = axes[1].imshow(\n",
    "    prob_grid.T, origin='lower', aspect='equal',\n",
    "    extent=[-5, 5, -5, 5],\n",
    "    cmap='hot'\n",
    ")\n",
    "plt.colorbar(im2, ax=axes[1], label='p(x)')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Probability Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e1c37",
   "metadata": {},
   "source": [
    "## 6. Conditional Flow Matching for p(g|s)\n",
    "\n",
    "In FlowShield-UDRL, we condition the flow on the state $s$ to model $p(g|s)$.\n",
    "This tells us what commands are achievable from a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d68cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conditional Flow Matching model using our actual implementation\n",
    "from src.models.safety.flow_matching import FlowMatchingModel\n",
    "\n",
    "cond_model = FlowMatchingModel(\n",
    "    data_dim=2,          # command dimension (horizon, return)\n",
    "    condition_dim=2,     # state dimension\n",
    "    hidden_dim=128,\n",
    "    n_layers=4,\n",
    "    n_integration_steps=100,\n",
    ").to(device)\n",
    "\n",
    "print(f'Parameters: {sum(p.numel() for p in cond_model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic state-conditioned command distribution\n",
    "def sample_state_conditioned_data(n_samples):\n",
    "    \"\"\"Sample (state, command) pairs where commands depend on state.\"\"\"\n",
    "    # States in a grid\n",
    "    states = torch.rand(n_samples, 2) * 4 - 2  # [-2, 2]^2\n",
    "    \n",
    "    # Commands depend on state:\n",
    "    # - Horizon depends on distance to center\n",
    "    # - Return depends on state position\n",
    "    distance = torch.norm(states, dim=-1)\n",
    "    \n",
    "    horizon = 10 + 5 * distance + torch.randn(n_samples) * 2\n",
    "    target_return = 20 - 3 * states[:, 0] + 2 * states[:, 1] + torch.randn(n_samples) * 3\n",
    "    \n",
    "    commands = torch.stack([horizon, target_return], dim=-1)\n",
    "    \n",
    "    return states, commands\n",
    "\n",
    "# Sample training data\n",
    "train_states, train_commands = sample_state_conditioned_data(5000)\n",
    "\n",
    "print(f'States shape: {train_states.shape}')\n",
    "print(f'Commands shape: {train_commands.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the state-command relationship\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# State distribution\n",
    "axes[0].scatter(train_states[:, 0], train_states[:, 1], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('State x')\n",
    "axes[0].set_ylabel('State y')\n",
    "axes[0].set_title('State Distribution')\n",
    "\n",
    "# Command distribution colored by state x\n",
    "sc = axes[1].scatter(train_commands[:, 0], train_commands[:, 1], \n",
    "                     c=train_states[:, 0], cmap='coolwarm', alpha=0.5, s=10)\n",
    "plt.colorbar(sc, ax=axes[1], label='State x')\n",
    "axes[1].set_xlabel('Horizon')\n",
    "axes[1].set_ylabel('Target Return')\n",
    "axes[1].set_title('Commands (colored by state x)')\n",
    "\n",
    "# Horizon vs distance to center\n",
    "distances = torch.norm(train_states, dim=-1)\n",
    "axes[2].scatter(distances, train_commands[:, 0], alpha=0.3, s=10)\n",
    "axes[2].set_xlabel('Distance from center')\n",
    "axes[2].set_ylabel('Horizon')\n",
    "axes[2].set_title('Horizon vs State Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train conditional flow matching\n",
    "optimizer = torch.optim.Adam(cond_model.parameters(), lr=1e-3)\n",
    "batch_size = 256\n",
    "n_epochs = 200\n",
    "\n",
    "train_states_dev = train_states.to(device)\n",
    "train_commands_dev = train_commands.to(device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Random batch\n",
    "    idx = torch.randint(0, len(train_states), (batch_size,))\n",
    "    states = train_states_dev[idx]\n",
    "    commands = train_commands_dev[idx]\n",
    "    \n",
    "    loss = cond_model.compute_loss(commands, states)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch {epoch+1}: Loss = {loss.item():.6f}')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Flow Matching Loss')\n",
    "plt.title('Conditional Flow Matching Training')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample commands for different states\n",
    "test_states = torch.tensor([\n",
    "    [-1.5, -1.5],  # bottom-left\n",
    "    [0.0, 0.0],    # center\n",
    "    [1.5, 1.5],    # top-right\n",
    "]).to(device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (state, ax) in enumerate(zip(test_states, axes)):\n",
    "    # Sample commands for this state\n",
    "    state_batch = state.unsqueeze(0).expand(200, -1)\n",
    "    with torch.no_grad():\n",
    "        sampled_commands = cond_model.sample(state_batch)\n",
    "    \n",
    "    # Get true commands for similar states\n",
    "    distances = torch.norm(train_states - state.cpu(), dim=-1)\n",
    "    mask = distances < 0.5\n",
    "    true_commands = train_commands[mask]\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(sampled_commands[:, 0].cpu(), sampled_commands[:, 1].cpu(),\n",
    "               alpha=0.5, s=20, c='blue', label='Sampled')\n",
    "    ax.scatter(true_commands[:, 0], true_commands[:, 1],\n",
    "               alpha=0.5, s=20, c='red', label='True')\n",
    "    ax.set_xlabel('Horizon')\n",
    "    ax.set_ylabel('Target Return')\n",
    "    ax.set_title(f'State = ({state[0]:.1f}, {state[1]:.1f})')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Conditional Sampling: True vs Generated Commands', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1e01b",
   "metadata": {},
   "source": [
    "## 7. OOD Detection with Flow Matching\n",
    "\n",
    "Commands with low probability under $p(g|s)$ are out-of-distribution (OOD)\n",
    "and potentially dangerous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a31963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log probability for a grid of commands conditioned on a state\n",
    "test_state = torch.tensor([[0.0, 0.0]]).to(device)  # Center state\n",
    "\n",
    "# Create command grid\n",
    "h_range = torch.linspace(5, 25, 40)\n",
    "r_range = torch.linspace(10, 30, 40)\n",
    "H, R = torch.meshgrid(h_range, r_range, indexing='xy')\n",
    "command_grid = torch.stack([H.flatten(), R.flatten()], dim=-1).to(device)\n",
    "\n",
    "# Expand state to match\n",
    "state_expanded = test_state.expand(len(command_grid), -1)\n",
    "\n",
    "# Compute log probs\n",
    "print('Computing log probabilities...')\n",
    "with torch.no_grad():\n",
    "    log_probs = cond_model.log_prob(command_grid, state_expanded)\n",
    "\n",
    "log_probs_grid = log_probs.reshape(40, 40).cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Log probability heatmap\n",
    "im1 = axes[0].imshow(\n",
    "    log_probs_grid.T, origin='lower', aspect='auto',\n",
    "    extent=[5, 25, 10, 30],\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.colorbar(im1, ax=axes[0], label='log p(g|s)')\n",
    "axes[0].set_xlabel('Horizon')\n",
    "axes[0].set_ylabel('Target Return')\n",
    "axes[0].set_title('Log Probability of Commands')\n",
    "\n",
    "# OOD detection (threshold-based)\n",
    "threshold = np.percentile(log_probs_grid, 10)  # 10th percentile as threshold\n",
    "ood_grid = log_probs_grid < threshold\n",
    "\n",
    "im2 = axes[1].imshow(\n",
    "    (~ood_grid).astype(float).T, origin='lower', aspect='auto',\n",
    "    extent=[5, 25, 10, 30],\n",
    "    cmap='RdYlGn', vmin=0, vmax=1\n",
    ")\n",
    "axes[1].set_xlabel('Horizon')\n",
    "axes[1].set_ylabel('Target Return')\n",
    "axes[1].set_title('Safe (green) vs OOD (red) Commands')\n",
    "\n",
    "# Add training data points\n",
    "nearby_mask = torch.norm(train_states, dim=-1) < 0.5\n",
    "nearby_commands = train_commands[nearby_mask]\n",
    "axes[0].scatter(nearby_commands[:, 0], nearby_commands[:, 1], c='red', s=10, alpha=0.3)\n",
    "\n",
    "plt.suptitle('OOD Detection for State (0, 0)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c6382",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways about Flow Matching for FlowShield:\n",
    "\n",
    "1. **Learns smooth probability flows**: Transforms Gaussian → complex distribution\n",
    "2. **Efficient training**: Simple MSE loss on vector field predictions\n",
    "3. **Exact density estimation**: Uses change of variables formula\n",
    "4. **Conditional modeling**: $p(g|s)$ tells us achievable commands per state\n",
    "5. **OOD detection**: Low probability ⟹ command is out-of-distribution\n",
    "\n",
    "For more details, see our full FlowShield-UDRL implementation!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
